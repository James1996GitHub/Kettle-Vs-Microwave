{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Object Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/James1996GitHub/Kettle-Vs-Microwave/blob/main/CNN%20Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdCZ8hpKhcNh"
      },
      "source": [
        "Mounts our Googled drive envirement to the session so we can import data in stroed in our Google drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bRnWoP3IL0g",
        "outputId": "647f6263-4b45-4eca-ac88-61191d809c7a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJD2qedUhts9"
      },
      "source": [
        "Imports all the libaries we will need to run this script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HfEMOzj3oMr"
      },
      "source": [
        "import os \n",
        "import pandas as pd\n",
        "import csv\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "\n",
        "import sklearn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkRpFhMAobFN"
      },
      "source": [
        "print('Is using GPU?', tf.test.is_gpu_available())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XD0FAWWTh8F7"
      },
      "source": [
        "Saving the locations of our image data from google drive so they can be called more conviently later on. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0NvqirT3zeg",
        "outputId": "7d59e60c-e0cb-40a1-cd22-28bc2d18092c"
      },
      "source": [
        "trainPath = '/content/drive/MyDrive/Data/TrainingData'\n",
        "testPath = '/content/drive/MyDrive/Data/TestingData/Data'\n",
        "kettlesPath = '/content/drive/MyDrive/Data/TrainingData/Kettle'\n",
        "microwavePath = '/content/drive/MyDrive/Data/TrainingData/Microwave'\n",
        "numKettles = len(os.listdir(kettlesPath))\n",
        "numMicrowaves = len(os.listdir(microwavePath))\n",
        "numTestImages = len(os.listdir(testPath))\n",
        "print('Number of kettles:', numKettles)\n",
        "print('Number of microwaves:', numMicrowaves)\n",
        "print('Number of test images:', numTestImages)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of kettles: 201\n",
            "Number of microwaves: 109\n",
            "Number of test images: 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_SD_nG6iY7D"
      },
      "source": [
        "Saving the locations of our CSV data from Google drive so they can be called more conviently later on. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 979
        },
        "id": "DT3kH1KX6wsx",
        "outputId": "6afd68bf-e1fc-4e7a-ba0d-52d654bb4635"
      },
      "source": [
        "testCSVPath = '/content/drive/MyDrive/Data/TestingData/Test - Sheet1.csv'\n",
        "testDataShape = pd.read_csv(testCSVPath,names= ['Image File'],skiprows=1)\n",
        "testDataShape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image File</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Kettle1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Kettle2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Kettle3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Kettle4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Kettle5.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Kettle6.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Kettle7.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Kettle8.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Kettle9.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Kettle10.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Kettle11.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Kettle12.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Kettle13.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Kettle14.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Kettle15.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Microwave1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Microwave2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Microwave3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Microwave4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Microwave5.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Microwave6.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Microwave7.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Microwave8.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Microwave9.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Microwave10.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>Microwave11.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>Microwave12.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Microwave13.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Microwave14.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Microwave15.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Image File\n",
              "0       Kettle1.jpg\n",
              "1       Kettle2.jpg\n",
              "2       Kettle3.jpg\n",
              "3       Kettle4.jpg\n",
              "4       Kettle5.jpg\n",
              "5       Kettle6.jpg\n",
              "6       Kettle7.jpg\n",
              "7       Kettle8.jpg\n",
              "8       Kettle9.jpg\n",
              "9      Kettle10.jpg\n",
              "10     Kettle11.jpg\n",
              "11     Kettle12.jpg\n",
              "12     Kettle13.jpg\n",
              "13     Kettle14.jpg\n",
              "14     Kettle15.jpg\n",
              "15   Microwave1.jpg\n",
              "16   Microwave2.jpg\n",
              "17   Microwave3.jpg\n",
              "18   Microwave4.jpg\n",
              "19   Microwave5.jpg\n",
              "20   Microwave6.jpg\n",
              "21   Microwave7.jpg\n",
              "22   Microwave8.jpg\n",
              "23   Microwave9.jpg\n",
              "24  Microwave10.jpg\n",
              "25  Microwave11.jpg\n",
              "26  Microwave12.jpg\n",
              "27  Microwave13.jpg\n",
              "28  Microwave14.jpg\n",
              "29  Microwave15.jpg"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS6X6ZD2BxAu"
      },
      "source": [
        "Setting our hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Zx_D2GHOncQ"
      },
      "source": [
        "epochs = 25 #Number of times will will cycle our data through the CNN during training.\n",
        "batchSize = 16 #Number of images passed though at a time.\n",
        "inputSize = (224,224) #Size images need to be resized to inorder to fit into the CNN input layer. \n",
        "classes = ['Kettle','Microwave'] #Defining the groups we want our CNN to sort its preditions into."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfA8vKcefegw"
      },
      "source": [
        "Here we are defining the preprocessing steps we need carried out on our raw image data so that it can be reformated into a format that can passed into our CNNs input player for training. This includes data augmentation techniques that are used to provide extra diversity in the dataset for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Y54IoVbOrhI",
        "outputId": "03e47cef-4968-4809-87e7-4f9fd522e2ab"
      },
      "source": [
        "trainImagePreprocessing=ImageDataGenerator(preprocessing_function=tf.keras.applications.inception_v3.preprocess_input,\n",
        "                                            shear_range=0.2,\n",
        "                                            rotation_range=45,\n",
        "                                            fill_mode='reflect',\n",
        "                                            validation_split=0.3,\n",
        "                                            horizontal_flip = True)\n",
        "\n",
        "testImagePreprocessing=ImageDataGenerator(preprocessing_function=tf.keras.applications.inception_v3.preprocess_input)\n",
        "\n",
        "#training generator\n",
        "trainGenerator=trainImagePreprocessing.flow_from_directory(directory= trainPath,\n",
        "                                            target_size = inputSize,\n",
        "                                            batch_size = batchSize,\n",
        "                                            classes = classes,\n",
        "                                            class_mode = \"categorical\",\n",
        "                                            subset = \"training\",\n",
        "                                            shuffle = True,\n",
        "                                            seed = False)\n",
        "\n",
        "#validation generator\n",
        "validationGenerator=trainImagePreprocessing.flow_from_directory(directory= trainPath,\n",
        "                                            target_size = inputSize,\n",
        "                                            batch_size = batchSize,\n",
        "                                            classes = classes,\n",
        "                                            class_mode = \"categorical\",\n",
        "                                            subset = \"validation\",\n",
        "                                            shuffle = True,\n",
        "                                            seed = False)\n",
        "#test generator\n",
        "testGenerator = testImagePreprocessing.flow_from_dataframe(dataframe=testDataShape,\n",
        "                                            directory=testPath,\n",
        "                                            x_col='Image File',\n",
        "                                            y_col=None,\n",
        "                                            target_size= inputSize,\n",
        "                                            batch_size=30,\n",
        "                                            class_mode=None,\n",
        "                                            shuffle= False,\n",
        "                                            validate_filenames=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 218 images belonging to 2 classes.\n",
            "Found 92 images belonging to 2 classes.\n",
            "Found 30 validated image filenames.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQMZzTtb_hLu",
        "outputId": "ad49961a-bef5-465d-d66d-80b398f3dafe"
      },
      "source": [
        "testGenerator"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.preprocessing.image.DataFrameIterator at 0x7f433dec20d0>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz86FcWqikmo"
      },
      "source": [
        "Loading the CNN model. This model has its initial layers pretained on data set from the 'Imagenet' database. We are only using our data to train the later convolutional layers of the network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss4WGIxOzGyC"
      },
      "source": [
        "base_model = tf.keras.applications.inception_v3.InceptionV3(weights=\"imagenet\",\n",
        "                                                            include_top=False)\n",
        "avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
        "output = tf.keras.layers.Dense(2,activation=\"softmax\")(avg)\n",
        "model = tf.keras.Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "for layer in base_model.layers:\n",
        "  layer.tranable = False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKNrrDuHiwPP"
      },
      "source": [
        "Defining the mertics we want to be caulated and displayed during training along with the optimizer chosen to minimise the loss value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u42C0hLT3t4V"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', \n",
        "              metrics=[tf.keras.metrics.Precision()],\n",
        "              optimizer = 'sgd')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMjak-j8fHMe"
      },
      "source": [
        "Kicks off the training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzCq4NHK_Z7H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a800812-0b41-46f2-e1c6-08b3df3cd54a"
      },
      "source": [
        "STEP_SIZE_TRAIN=trainGenerator.n//trainGenerator.batch_size\n",
        "STEP_SIZE_VALID=validationGenerator.n//validationGenerator.batch_size\n",
        "birdBrain = model.fit_generator(generator=trainGenerator,\n",
        "                              steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "                              validation_data=validationGenerator,\n",
        "                              validation_steps=STEP_SIZE_VALID,\n",
        "                              epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "13/13 [==============================] - 113s 8s/step - loss: 0.6941 - precision_1: 0.5891 - val_loss: 0.6892 - val_precision_1: 0.5500\n",
            "Epoch 2/25\n",
            "13/13 [==============================] - 102s 8s/step - loss: 0.5068 - precision_1: 0.7376 - val_loss: 0.7725 - val_precision_1: 0.7125\n",
            "Epoch 3/25\n",
            "13/13 [==============================] - 102s 8s/step - loss: 0.4235 - precision_1: 0.8020 - val_loss: 0.7264 - val_precision_1: 0.6125\n",
            "Epoch 4/25\n",
            "13/13 [==============================] - 101s 8s/step - loss: 0.3391 - precision_1: 0.8960 - val_loss: 0.7259 - val_precision_1: 0.7625\n",
            "Epoch 5/25\n",
            "13/13 [==============================] - 101s 8s/step - loss: 0.3060 - precision_1: 0.8812 - val_loss: 0.3832 - val_precision_1: 0.7875\n",
            "Epoch 6/25\n",
            "13/13 [==============================] - 101s 8s/step - loss: 0.2231 - precision_1: 0.9208 - val_loss: 0.5748 - val_precision_1: 0.7000\n",
            "Epoch 7/25\n",
            "13/13 [==============================] - 101s 8s/step - loss: 0.1637 - precision_1: 0.9554 - val_loss: 0.5059 - val_precision_1: 0.8250\n",
            "Epoch 8/25\n",
            "13/13 [==============================] - 101s 8s/step - loss: 0.1621 - precision_1: 0.9505 - val_loss: 0.3301 - val_precision_1: 0.8625\n",
            "Epoch 9/25\n",
            "13/13 [==============================] - 100s 8s/step - loss: 0.1456 - precision_1: 0.9554 - val_loss: 0.4222 - val_precision_1: 0.8125\n",
            "Epoch 10/25\n",
            "13/13 [==============================] - 101s 8s/step - loss: 0.0884 - precision_1: 0.9752 - val_loss: 0.3615 - val_precision_1: 0.8750\n",
            "Epoch 11/25\n",
            "13/13 [==============================] - 101s 8s/step - loss: 0.1017 - precision_1: 0.9554 - val_loss: 0.4749 - val_precision_1: 0.7875\n",
            "Epoch 12/25\n",
            "13/13 [==============================] - 100s 8s/step - loss: 0.0797 - precision_1: 0.9851 - val_loss: 0.3900 - val_precision_1: 0.9125\n",
            "Epoch 13/25\n",
            "13/13 [==============================] - 101s 8s/step - loss: 0.0448 - precision_1: 0.9950 - val_loss: 0.3451 - val_precision_1: 0.8000\n",
            "Epoch 14/25\n",
            "13/13 [==============================] - 100s 8s/step - loss: 0.0801 - precision_1: 0.9851 - val_loss: 0.5148 - val_precision_1: 0.8000\n",
            "Epoch 15/25\n",
            "13/13 [==============================] - 103s 8s/step - loss: 0.0564 - precision_1: 0.9856 - val_loss: 0.4606 - val_precision_1: 0.8375\n",
            "Epoch 16/25\n",
            "13/13 [==============================] - 101s 8s/step - loss: 0.0462 - precision_1: 0.9901 - val_loss: 0.3679 - val_precision_1: 0.8500\n",
            "Epoch 17/25\n",
            "13/13 [==============================] - 101s 8s/step - loss: 0.0606 - precision_1: 0.9851 - val_loss: 0.3548 - val_precision_1: 0.8625\n",
            "Epoch 18/25\n",
            "13/13 [==============================] - 101s 8s/step - loss: 0.0438 - precision_1: 0.9802 - val_loss: 0.3125 - val_precision_1: 0.8375\n",
            "Epoch 19/25\n",
            "13/13 [==============================] - 101s 8s/step - loss: 0.0152 - precision_1: 1.0000 - val_loss: 0.4016 - val_precision_1: 0.8500\n",
            "Epoch 20/25\n",
            "13/13 [==============================] - 101s 8s/step - loss: 0.0434 - precision_1: 0.9802 - val_loss: 0.4476 - val_precision_1: 0.8375\n",
            "Epoch 21/25\n",
            "13/13 [==============================] - 101s 8s/step - loss: 0.0391 - precision_1: 0.9950 - val_loss: 0.1783 - val_precision_1: 0.9250\n",
            "Epoch 22/25\n",
            "13/13 [==============================] - 101s 8s/step - loss: 0.0293 - precision_1: 0.9950 - val_loss: 0.3999 - val_precision_1: 0.8750\n",
            "Epoch 23/25\n",
            "13/13 [==============================] - 101s 8s/step - loss: 0.0492 - precision_1: 0.9901 - val_loss: 0.2554 - val_precision_1: 0.8875\n",
            "Epoch 24/25\n",
            "13/13 [==============================] - 101s 8s/step - loss: 0.0467 - precision_1: 0.9752 - val_loss: 0.2865 - val_precision_1: 0.8875\n",
            "Epoch 25/25\n",
            "13/13 [==============================] - 101s 8s/step - loss: 0.0552 - precision_1: 0.9802 - val_loss: 0.1695 - val_precision_1: 0.9125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9g7SRu6jddvV"
      },
      "source": [
        "Saving the weights of out trained model so that it can be recalled outside of the current session withought having to retrain the weights from scratch. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01-9tOE4qESn"
      },
      "source": [
        "model.save('/content/drive/MyDrive/birdBrainModel/InceptionV3/birdBrainModel.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeHVdhwRsp_i"
      },
      "source": [
        "Loads the weights of the cnn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Na4hPG9cPd_"
      },
      "source": [
        "birdBrain = tf.keras.models.load_model('/content/drive/MyDrive/birdBrainModel/InceptionV3/birdBrainModel.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2voM5Fl9szdw"
      },
      "source": [
        "Here we are applying our trained model to the test data and it is returning a level of confidance for each class.\n",
        "\n",
        "[kettle, microwave]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBM7q_VHv6_t",
        "outputId": "0d34ef1a-fa37-49d3-8c53-fed705b55e8a"
      },
      "source": [
        "ypred = birdBrain.predict(testGenerator,batchSize)\n",
        "print(ypred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[9.7562510e-01 2.4374954e-02]\n",
            " [1.7422998e-01 8.2577002e-01]\n",
            " [5.5844712e-01 4.4155288e-01]\n",
            " [8.5438985e-01 1.4561018e-01]\n",
            " [9.9924952e-01 7.5050775e-04]\n",
            " [2.9160764e-02 9.7083920e-01]\n",
            " [1.4512222e-01 8.5487777e-01]\n",
            " [9.8549473e-01 1.4505215e-02]\n",
            " [9.9377948e-01 6.2204786e-03]\n",
            " [9.5442063e-01 4.5579433e-02]\n",
            " [9.9848539e-01 1.5146134e-03]\n",
            " [9.9544626e-01 4.5537357e-03]\n",
            " [9.9838579e-01 1.6142568e-03]\n",
            " [9.7906452e-01 2.0935535e-02]\n",
            " [7.5653440e-01 2.4346562e-01]\n",
            " [8.8072336e-01 1.1927665e-01]\n",
            " [9.4780976e-01 5.2190237e-02]\n",
            " [7.8882225e-02 9.2111784e-01]\n",
            " [8.2941301e-02 9.1705865e-01]\n",
            " [3.5712382e-04 9.9964285e-01]\n",
            " [1.0872863e-01 8.9127141e-01]\n",
            " [2.1026336e-01 7.8973663e-01]\n",
            " [1.7505317e-03 9.9824947e-01]\n",
            " [1.3541401e-02 9.8645860e-01]\n",
            " [3.1505519e-01 6.8494481e-01]\n",
            " [2.1822723e-03 9.9781775e-01]\n",
            " [2.9627133e-02 9.7037292e-01]\n",
            " [7.2477162e-02 9.2752278e-01]\n",
            " [1.0128146e-02 9.8987192e-01]\n",
            " [7.2455889e-04 9.9927551e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpU8mNPZs855"
      },
      "source": [
        "The two rows representing predidictions are checked and the row with the larger confidance is returned. Kettle = 0 as it was row 0 and microwave = 1 as it is row 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lq4o0X3PPEhJ",
        "outputId": "c35f08f6-10e5-4479-e524-4bcf08a37376"
      },
      "source": [
        "predictions = np.argmax(ypred, axis = 1)\n",
        "print(predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj9uSEapxKIY"
      },
      "source": [
        "Defining what answser we know should be returned from our test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PER4lsfKrjiL"
      },
      "source": [
        "testLables = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtcHlWcEcvba"
      },
      "source": [
        "Here we are defining the method that we can call upon to polt our Confusion Matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhh-wn7ofKAy"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, ['Kettles Predicted','Microwaves Predicted'], rotation=45)\n",
        "    plt.yticks(tick_marks, ['Kettles Labled','Microwaves Labled'])\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eelWlayjcZaL"
      },
      "source": [
        "This section of code polts and displays the Confusion Matrix by calling the function we defined in the code snippit above.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "JX0m0Busii9O",
        "outputId": "c08afac5-8e81-41db-8737-44c7037f6a1d"
      },
      "source": [
        "cm = confusion_matrix(y_true=testLables, y_pred=predictions)\n",
        "plot_confusion_matrix(cm=cm, classes=classes,title='Confusion Matrix')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[12  3]\n",
            " [ 2 13]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrnK9cSU_xC4"
      },
      "source": [
        "This section applies the CNN to 1 single image and determins the class of that image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tlkVsPeYvy6",
        "outputId": "33e366a6-6657-40b6-f961-28425baf6d88"
      },
      "source": [
        "imPath = '/content/drive/MyDrive/Data/TestingData/Data/Kettle8.jpg'\n",
        "# convert image to PIL image\n",
        "image = tf.keras.preprocessing.image.load_img(imPath)\n",
        "# convert image to numpy array\n",
        "input_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
        "# convert single image to a batch\n",
        "input_arr = np.array([input_arr])\n",
        "# apply our preprocessing to the array to fit CNN input layer\n",
        "input_arr = tf.keras.applications.inception_v3.preprocess_input(input_arr)\n",
        "# applies the trained model to \n",
        "ypred = birdBrain.predict(input_arr)\n",
        "# determins most likley class and prints the results by comparing the rows of the ouputted matrix\n",
        "prediction = np.argmax(ypred, axis=1)\n",
        "print(ypred)\n",
        "print(prediction)\n",
        "if prediction == 0:\n",
        "  print('Kettle detected')\n",
        "elif prediction == 1:\n",
        "  print('Microwave detected')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.5373729  0.46262705]]\n",
            "[0]\n",
            "Kettle detected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lWJ54ZOEtEF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}